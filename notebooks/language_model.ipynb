{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Dropbox/programs/critic\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Dropbox/programs/critic\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import rho_plus as rp\n",
    "import wat\n",
    "\n",
    "is_dark = False\n",
    "theme, cs = rp.mpl_setup(is_dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "CUDA backend failed to initialize: Unable to use CUDA because of the following issues with CUDA components:\n",
      "Unable to load cuDNN. Is it installed?\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicholas/anaconda3/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 322, in _version_check\n",
      "    version = get_version()\n",
      "              ^^^^^^^^^^^^^\n",
      "RuntimeError: cuDNN not found.\n",
      " (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/nicholas/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, FlaxAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = 'distilbert/distilgpt2'\n",
    "model = FlaxAutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([  256, 11626,  5509,  8812,   304, 30479,  1059,   612,   384,\n",
       "       15867,   256,  1059, 26197,  3381,  5019,   994,   547],      dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "option_strs = ['tere', 'tear', 'tree', 'terr', 'ere', 'tee', 'terse', 'there', 'sere', 'tire', 'tare', 'tern', 'tore', 'term', 'mere', 'here', 'were']\n",
    "options = jnp.array([x[0] for x in tokenizer.batch_encode_plus([' ' + x for x in option_strs])['input_ids']])\n",
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-59.337803, -57.187347, -63.34384 , ..., -66.71974 , -64.19488 ,\n",
       "       -60.27142 ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"That's neither here nor\", return_tensors=\"np\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# retrieve logts for next token\n",
    "next_token_logits = outputs.logits[:, -1][0]\n",
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there    0.090\n",
      "here     0.063\n",
      "were     0.056\n",
      "se       0.056\n",
      "t        0.056\n",
      "t        0.056\n",
      "e        0.056\n",
      "mere     0.056\n",
      "ter      0.056\n",
      "ter      0.056\n",
      "term     0.056\n",
      "tear     0.056\n",
      "tree     0.056\n",
      "terr     0.056\n",
      "tee      0.056\n",
      "tire     0.056\n",
      "dtype: float32\n",
      "there        0.470\n",
      "in           0.150\n",
      "here         0.103\n",
      "anywhere     0.061\n",
      "at           0.017\n",
      "the          0.014\n",
      "now          0.014\n",
      "on           0.011\n",
      "elsewhere    0.010\n",
      "yet          0.010\n",
      "ever         0.006\n",
      "abroad       0.005\n",
      "any          0.005\n",
      "I            0.004\n",
      "a            0.003\n",
      "near         0.003\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "probs = jax.nn.softmax(next_token_logits)\n",
    "\n",
    "option_probs = jax.nn.softmax(probs[options])\n",
    "\n",
    "values, indices = jax.lax.top_k(option_probs, k=16)\n",
    "tokens = tokenizer.batch_decode(options[indices])\n",
    "results = pd.Series(values, index=tokens)\n",
    "print(results.round(3))\n",
    "\n",
    "values, indices = jax.lax.top_k(probs, k=16)\n",
    "tokens = tokenizer.batch_decode(indices)\n",
    "results = pd.Series(values, index=tokens)\n",
    "print(results.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
